import urllib.request
from bs4 import BeautifulSoup

import re
import unidecode

# Custom Python files to generate JSON and PDF
import generateJSON
import generateReport



# Initialize the New Yorker website resources on which you want to carry out data analysis
# Initialiser les ressources du site Web de New York sur lesquelles vous souhaitez effectuer l'analyse des données



theurl = "https://www.newyorker.com/contributors"
thepage = urllib.request.urlopen(theurl)
soup = BeautifulSoup(thepage,"html.parser")


# initialize the variables necessary for the analysis.
# initialiser les variables nécessaires à l'analyse.



contributors=[]
no_of_articles_for_the_author=0;
contributors_links=[];
var_for_no_hold_articles= [];
var_for_no_hold_articles_phrase= [];
contributor_description=[];


# Scrap for all the contributors at New Yorker. So for find all the list items of class : Contributors__name___KjAyf and adding it in variable and getting all
# Scrap pour tous les contributeurs à New Yorker, Donc, pour trouver tous les éléments de la liste de la classe: Contributors__name___KjAyf et l'ajouter à la variable et obtenir tous


for allContributors in soup.findAll('li',{"class":"Contributors__name___KjAyf"}):
    if allContributors.find('a').text not in contributors:
        contributors.append(re.sub('\s+', '-',allContributors.find('a').text))


# this module you get each contributor's website @ New Yorker
# ce module, vous obtenez le site Web de chaque contributeur @ New Yorker


for i in range(0,len(contributors)):
    contributors_links.append("https://www.newyorker.com/contributors/" + re.sub('[!@#$,.]',"", unidecode.unidecode(contributors[i]).lower()))


# Now , in this module we get how many articles are written by the individual contributors
# Maintenant, dans ce module, nous recevons combien d'articles ont été écrits par les contributeurs individuels


# the line below traverses through each contributors website to get info about how many articles are written by the contributors
# Now , i used "unidecode.unidecode(contributors[go_through]).lower())" because there in one link that is generated by the above code
# Doreen-St.-Félix https://www.newyorker.com/contributors/Doreen-St.-Félix
# this auto generated website has no link, so hence i had to use ""unidecode.unidecode(contributors[go_through]).lower())"

# la ligne ci-dessous traverse le site Web de chaque contributeur pour obtenir des informations sur le nombre d'articles écrits par les contributeurs
# Maintenant, j'ai utilisé "unidecode.unidecode (contributors [go_through]). Lower ())" car il y a dans un lien qui est généré par le code ci-dessus
# Doreen-St.-Félix https://www.newyorker.com/contributors/Doreen-St.-Félix
# Ce site Web généré automatiquement n'a pas de lien, je devais donc utiliser "" unidecode.unidecode (contributors [go_through]). lower ()) "

for go_through in range(0,len(contributors)):
    print("Working for "+str(go_through)+" out of "+str(len(contributors)))

    theurl = "https://www.newyorker.com/contributors/" + re.sub('[!@#$,.]',"", unidecode.unidecode(contributors[go_through]).lower())
    thepage = urllib.request.urlopen(theurl)
    soup = BeautifulSoup(thepage, "html.parser")


    # this will collect the bio of individual authors
    # cela recueillera la bio des auteurs individuels

    if soup.find('div',{"class":"ContributorBio__contributorBio___AF0ID"})is None:
        contributor_description.append(" ")
    else:
        contributor_description.append(str(soup.find('div',{"class":"ContributorBio__contributorBio___AF0ID"}).find('div').text.encode("ascii", "ignore")).replace('\\n',' '))



    # this will collect the no of articles written by individual authors
    # ceci collectera le nombre d'articles écrits par des auteurs individuels


    for noofarticles in soup.find('ul',{"class":"Pagination__list___1xXbl"}).findAll('li',{"class":"Pagination__listItem___1hFiK"}):
        if noofarticles.find('span'):
            dummy_useless=""
        else:
            var_for_no_hold_articles = noofarticles.find('a').text

    var_for_no_hold_articles_phrase.append(contributors[go_through] +" has written about " + str(int(var_for_no_hold_articles) * 10) + "+ articles")



# This will generate the JSON file called data.json with all the details collected above
# Cela générera le fichier JSON appelé data.json avec tous les détails collectés ci-dessus

generateJSON.generateJSONFile( contributors, var_for_no_hold_articles_phrase, contributors_links, contributor_description)

# This will generate the report(PDF) file called NewYorkerReport.pdf  with all the details collected above
# Ceci générera le rapport (PDF) fichier appelé NewYorkerReport.pdf avec tous les détails collectés ci-dessus

generateReport.createPDFReport( contributors, var_for_no_hold_articles_phrase, contributors_links, contributor_description)















